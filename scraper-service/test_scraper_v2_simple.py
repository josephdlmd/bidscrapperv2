"""
Simple test of PhilGEPS Scraper V2
Tests scraping only (no database save)
"""
import asyncio
import os
import json
from scraper.philgeps_scraper import PhilGEPSScraper

# Set credentials from .env.example
os.environ['PHILGEPS_USERNAME'] = 'jdeleon60'
os.environ['PHILGEPS_PASSWORD'] = 'Merritmed#01'

async def main():
    print("="*80)
    print("ğŸ§ª TESTING PhilGEPS Scraper V2 - Scraping Only")
    print("="*80)
    print()
    print("ğŸ“‹ Test Configuration:")
    print("   - Max pages: 1 (first page only)")
    print("   - Fetch details: 1 bid only")
    print("   - Database: Skip saving (test scraping only)")
    print("   - Username:", os.environ['PHILGEPS_USERNAME'])
    print()

    scraper = PhilGEPSScraper()

    try:
        # PHASE 1: Test list scraping
        print("ğŸš€ PHASE 1: Testing list scraping...")
        bid_list = await scraper.scrape_current_opportunities_list(max_pages=1)

        if not bid_list:
            print("âŒ No bids found!")
            return

        print(f"\nâœ… Found {len(bid_list)} bids on first page")
        print("\nFirst 3 bids:")
        for i, bid in enumerate(bid_list[:3], 1):
            print(f"\n{i}. Bid #{bid['reference_number']}")
            print(f"   Title: {bid['title'][:60]}...")
            print(f"   Agency: {bid['agency_name']}")
            print(f"   Classification: {bid['classification']}")
            print(f"   Closing: {bid['closing_date']}")
            print(f"   Detail URL: {bid['detail_url']}")

        # PHASE 2: Test detail scraping on first bid only
        if len(bid_list) > 0:
            print("\n" + "="*80)
            print("ğŸš€ PHASE 2: Testing detail scraping (1 bid only)...")

            test_bid = bid_list[0]
            detail = await scraper.scrape_bid_detail(
                test_bid['reference_number'],
                test_bid['detail_url']
            )

            if detail:
                print(f"\nâœ… Successfully fetched details for bid #{detail['reference_number']}")
                print(f"\nğŸ“Š Detail Information:")
                print(f"   Approved Budget: {detail.get('approved_budget', 'N/A')}")
                print(f"   Delivery Period: {detail.get('delivery_period', 'N/A')}")
                print(f"   Contact Person: {detail.get('contact_person', 'N/A')}")
                print(f"   Delivery Location: {detail.get('delivery_location', 'N/A')}")
                print(f"   Business Category: {detail.get('business_category', 'N/A')}")

                if detail.get('line_items'):
                    print(f"\n   Line Items: {len(detail['line_items'])} items")
                    for item in detail['line_items'][:3]:
                        print(f"      - {item['lot_name']} (Qty: {item['quantity']} {item['unit_of_measure']})")
            else:
                print("âŒ Failed to fetch details")

        print("\n" + "="*80)
        print("âœ… TEST COMPLETED SUCCESSFULLY!")
        print("="*80)
        print(f"\nSummary:")
        print(f"   âœ“ List scraping works: {len(bid_list)} bids found")
        print(f"   âœ“ Detail scraping works: Budget and line items extracted")
        print(f"   âœ“ All selectors working correctly")
        print(f"\nğŸ’¡ Ready to run full scrape with database saving!")

    except Exception as e:
        print(f"\nâŒ Exception occurred: {e}")
        import traceback
        traceback.print_exc()
    finally:
        scraper.close()
        print("\nğŸ”’ Scraper closed")

if __name__ == "__main__":
    asyncio.run(main())
